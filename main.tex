\documentclass[11pt, oneside]{report}
\usepackage[utf8]{inputenc}

\title{Communicating Replicated State Machines}
\author{David Wetterau}
\date{April 2015}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}
Computing systems today are rarely self-contained units. The desire for more levels of abstraction and cleaner separation of responsibilities has led to systems being modularized into services that must communicate and work together to accomplish tasks. Not only is this separation observed within a single system, but also in systems that interact with external services such as cloud based storage or computing providers. In this paper we will present a way to build these highly available systems by leveraging existing solutions such as the abstraction of a replicated state machine.

Unfortunately, the replicated state machine abstraction as it exists today is not immediately fit for this multiple service environment. Previous work on the RSM abstraction has focused on the client-server model, where the RSM behaves as a single-correct-server from only the perspective of the client. This behavior does not allow these systems to communicate with other services because doing so could expose the intermediate state of the system and there is no guarantee that this intermediate state is consistent.

Aside from consistency issues, a traditional RSM abstraction would also suffer from fundamental performance deficiencies. Most replication protocols rely on the sequential execution of commands to guarantee that replicas operate in a deterministic manner. Not only does sequential execution do a poor job of leveraging today’s multicore machines, it also would be require a system in this environment to remain idle while waiting for the response from a command issued to another service.

In this paper, we will present a technique to adapt the replicated state machine abstraction to the environment described above. We will first leverage recent work on speculative execution to allow our system to better utilize the multicore nature of machines today. In addition, we will show how to pipeline a system using speculative execution to mitigate the blocking of the system while waiting for the response of communications it may perform. This work builds on operations originally made by the Laboratory for Advanced Systems Research at UT and was a joint effort between myself and Jim Given. I highly recommend reading Jim’s thesis on this same work to get the complete understanding of this project.

The rest of this paper is structured as follows. Chapter 2 will provide a comprehensive background on the replicated state machine abstraction and the Eve system, which pioneered the use of speculative execution within a RSM. Chapter 3 will introduce Adam, a system to provide high availability in an environment with services that communicate not only with clients, but also with other services. Chapter 4 will describe at a high level the design and benefits of pipelining in Adam. Chapter 5 will explore our implementation of Adam in detail along with the correctness guarantees Adam provides. Chapter 6 will display our evaluation and the performance of Adam. Chapter 7 will discuss work related to Adam and the conclusion of this thesis will be presented in Chapter 8.

\chapter{Background}
\section{State Machine Replication}
In order for a system to be highly available in the presence of failures, the system must be replicated. State Machine Replication (SMR) is a way to guarantee that multiple replicas, either virtual or physical, produce the same outputs when given the same commands. In this section we will explain the abstraction of a replicated state machine along with how SMR has been traditionally implemented.

At the core of SMR is a deterministic state machine. A state machine is a model of computation that consists of state variables, which store the state of the machine, and commands that transform this state. In SMR, requests are issued by clients and contain the name of the state machine to operate on, the command to perform, and any other information needed to perform the command. When a replica receives a request, it performs the command and as a result both produces an output and either transitions to a new state in the case of a write request, or returns to the same state it started at in the case of a read request.

The correctness goal of SMR is to provide a system that retains both safety and liveness in the presence of failures. SMR can be adapted to work under many different failure models simply by adjusting the number of replicas. For a weaker failure model, one only needs to either increase the number of replicas in the system or decrease the number of tolerated failures accordingly. In order to provide this correctness, the requirement of SMR is that the observable state (including but not limited to the outputs) of each state machine must be the same across all replicas.

To provide this correctness requirement, SMR has traditionally been implemented by only allowing sequential execution of commands. The idea behind this is that if commands are first ordered the same way on each replica, and are then executed one at a time in this deterministic way, then all replicas will arrive at the same state. This is easy to see because each state machine simply follows the exact same state transitions in the same order. This design is referred to as the agree-execute approach, and has been applied in a wide range of systems including the original Paxos protocol.

Sequential execution was an acceptable option when SMR was first introduced since single core performance was rapidly increasing. Now that the world has shifted to increasing the amount of parallelism to improve performance however, sequential execution is a serious limitation. If we tried to naively relax this sequential execution restriction and run commands using multiple threads, there would be no guarantee that the replicas would arrive in the same final state since the commands could be interleaved across the threads. In the next section, we will see a way that we can still provide the replicated state machine abstraction without the need for this sequential execution restriction.  

\section{Eve: Execute-Verify Replication}

In the previous section, we explored how the majority of SMR implementations relied on the agree-execute architecture to ensure that replicas converged to the same ending state. This approach guarantees convergence through determinism and sequential execution. These approaches have effectively reduced the actual safety requirement of SMR (that the observable state of each state machine is the same across all replicas) to the problem of guaranteeing that all replicas agree on an order of commands to execute.

Eve takes a completely different approach to providing the safety guarantee of SMR with speculative execution while still providing liveness to form a correct replicated state machine abstraction. In Eve, replicas are given a set, or batch, of requests to execute from the primary, however they are not required to agree on an order in which to execute them. Replicas then fully leverage the parallel potential of their underlying multicore machines to execute all of the requests speculatively. At the end of the execution, replicas compare states with each other through a verifier and if enough replicas arrived at the same state, the results of the client requests are released. If not enough replicas agree on the final state, it must be because of a non-deterministic interleaving of the requests, so each replica simply re-executes all of the requests in the group in a deterministic order sequentially to guarantee both safety and liveness.

In theory the execute-verify approach is very straightforward, but in order for Eve to work well, it's important that the case that requests have to be re-executed is rare. If this were not the case, Eve would be strictly worse than SMR implemented in a sequential manner becauase of the need to execute requests multiple times to guarantee safety. To reduce the likelihood that request interleavings cause the final states of replicas to diverge, the authors introduce what they call the mixer which is discussed in subsection 2.2.1. After Eve runs the mixer on the incoming requests, it then proceeds to the execution stage which is described in section 2.2.2. After executing a batch of requests, Eve moves on to the verification stage which guarantees that replicas compare their states efficiently and in a way that tolerates even Byzantine failures. This verification stage is described in subsection 2.2.3 and the final results that authors were able to produce with Eve are presented in subsection 2.2.4.

\subsection{The Mixer}
Given a set of requests to execute, the mixer constructs a batch of requests to execute in the following way. The mixer examines each request to determine if it conflicts with any request already added to the batch to execute. If the request does not conflict, it adds the request to the next batch to execute. If it does conflict, the mixer passes over the request. After a complete pass, the mixer releases the batch to be executed and after doing so, repeats this process on the requests that were left out of the batch on the previous pass.

The authors found that mixers were both easy to implement for various real applications such as the TPC-W benchmarks, and greatly reduced the likelihood that replicas would diverge. It is important to note that while the mixer does improve performance of the system greatly, it is not needed for either the safety or liveness conditions of SMR. Because Eve can re-execute requests as a fallback, there is no requirement that the requests in a batch do not conflict and therefore there is no required accuracy of the mixer in order for Eve to be correct.

\subsection{The Execution Stage}

The execution stage in Eve is where the system gains its massive performance gains over sequential SMR. The output of running the mixer on the set of client requests to execute is a batch of requests that are hopefully unlikely to cause problems when they are executed in parallel. In the execution stage, these requests are spread out across a number of execution threads to be run at the same time. When all of the execution threads finish performing the commands specified by the client requests in the batch, a hash of the final state of the state machine is computed efficiently using a concurrent Merkle tree implementation. This hash of the final state is then used in the verification stage to determine whether or not the parallel execution caused a divergence in the final states of the replicas.

\subsection{The Verification Stage}
In Eve's verification stage, replicas must compare their final states in an efficient manner that is also resistant to byzantine failures. This is accomplished by relying on cryptographic primitives such as Message Authentication Codes and collision resistant hashes of the states. As mentioned in the previous subsection, Eve replicas compute a hash of their state after processing a batch of requests by each using a Merkle tree for efficiency. After computing this hash, replicas sign and send to a separate set of verification machines a signed token that consists of the hash they started with before processing the current batch, and the hash of their state after processing the batch in a parallel manner.

The verification machines then run an agreement protocol on the tokens submitted to them by all the replicas. In the simple case, all of the tokens match and the verifiers instruct the primary to release the result of the computation to the clients by responding with a ``commit" message. If instead some tokens differ but there is a token that enough replicas submitted to guarantee that a correct replica submitted it, the verification machines respond with a commit for that token but require replicas that did not reach that state to receive a state transfer from the replicas that did. 

In the worse case, the verifiers do not identify a token that was reached by enough replicas so the verifiers require all replicas to re-execute the requests in the batch in a sequential manner. After the replicas have re-executed the request, there is no need to enter the verification stage again because the execution is done deterministically and guarantees that all replicas arrive at the same ending state. Note that this full stage maintains the liveness property, since it is impossible for the replicas to get stuck in an infinite loop of verification.

\subsection{Performance Results}

The experimental evaluation of Eve resulted in promising results for this approach to a parallelizable replicated state machine. In particular, the creators of Eve demonstrated with microbenchmarks that Eve was capable of a 12.5x speedup over sequential execution using 16 core machines with 10ms requests. These numbers fell slightly with lighter-weight requests. They found that the improvement decreased to 10x for 1ms / request and 3.3x for 0.1ms / request mainly due to the inability to saturate the system with client requests and the increased overhead relative to the time spent processing the request.

The authors also compared Eve's performance to an existing attempt at performing multi-threaded SMR called Remus. In Remus, the primary replica executes requests with multiple threads and then transfers its entire state to the other replicas. This approach, referred to as passive replication, both requires considerably more bandwidth (due to the transfer of full states) and does not protect against commission failures, while Eve does. Eve was shown to outperform Remus by a factor of 4.7x while using orders of magnitude less network bandwidth.

\chapter{Adam: State Machine Replication with Multiple Services}

In the last chapter we saw that the replicated state machine abstraction could be used with in a parallel environment with serious performance improvements in terms of request throughput. This approach successfully leverages the multi-core nature of today's machines, but it is not easily extended to allow for services to communicate at first glance. Systems today are rarely well described by the simple client-server model. Instead, most services need to interact with other services while processing a request from a client. 

For example, if a client visits the website Dropbox.com, the server handling the clients request needs to make a nested request to a separate service to retrieve the names of the client's files, before sending the contents of the webpage back to the client. In this chapter, we will explore the previous work that was performed to convert the Eve system to work in this new environment, one where servers not only expose their states to clients, but also to other services through nested requests. This project was named Adam by the the same group that presented the Eve system.

\section{Initial Approaches}

Before describing Adam, it is important to understand why speculative execution approaches such as Eve do not work in this environment with communicated services. Consider a service $A$ that is using Eve and is processing client requests. While processing some of the client requests service $A$ receives, it must make a call to service $B$. Now suppose that due to the speculative nature of Eve, too many replicas diverge while processing this client request and all replicas need to rollback to their state before and try again in a sequential manner. At this point, the request that service $A$ made to $B$ has already happened and cannot be rolled back. If service $B$ had clients of its own, they could see the result of this nested request even after service $A$ tried again. Worse still, when service $A$ re-executes the requests sequentially, there is no guarantee that it makes exactly the same nested request. Non-determinism caused by thread interleaving could result in a nested request made for the same client request that is different from the nested request that would be made when processing sequentially.

This well known problem is known as the output-commit problem. A very simple solution to the problem is to always take a snapshot before "outputting" or exposing internal state to any outside observer. The first attempt to convert Eve to work in this setting took exactly this approach, and is described in the following subsection. A separate idea to overcome this difficulty was to simply run in a deterministic and sequential manner, but to leverage deterministic pipelining for increased performance. With this idea, Adam would not be speculative and no rollback would ever be needed to maintain correctness.

\subsection{Eve Modification}

In an effort to leverage the performance gains that speculative execution approaches such as Eve promised, the first attempt to construct Adam focused on solving the output-commit problem that Eve faced in this environment. To do this, the 
BLANK %Idk what to put here to describe the UTCS people who worked on this first
simply modified the existing Eve implementation to take checkpoints before making nested requests. 

% Diagram here or something about all hitting the wall at the same time

Consider an Eve system with two execution threads processing client requests in a speculative manner. When the execution threads arrive at the part of processing the client request that requires a nested request to another service, they inform the system of their project and wait. After the last thread reaches such a point in the request processing, the system computes a hash of its state in exactly the same way that Eve ensured all replicas were the same at the end of processing an entire batch. If all replicas were in the same state after processing these prefixes of the client requests, the threads could safely make their nested requests and continue executing until the next such time, or until the end of their last request.

\subsection{Single-Threaded Pipelining}

A completely separate way to solve this output-commit problem is to simply not execute in a speculative manner. The
BLANK %Idk what to put here to describe the UTCS people who worked on this first
also implemented a system that leveraged single-threaded deterministic pipelining to try and solve this issue.

The key insight to the advantage of pipelining in this environment is that when a single thread that is processing client requests makes a request to a separate service, it must wait synchronously for the response. If this communication with the backend service takes as long as the computation that the first service must perform, this would mean that the system would be idle half the time. It is then clear that if it were possible to perform more work while waiting for the nested request's response, the performance of the system would increase.

In order to build a system that performed this pipelining, the BLANK used multiple threads but only ran them one at a time and scheduled them in a deterministic way. Deterministic scheduling and only having a single execution thread guaranteed that all requests were executed deterministically, and therefore that all replicas would converge in the system without the need for checkpointing or potentially rolling back. 

\section{Initial Results}

Then initial findings in terms of the performance gain that applying speculative execution had over a naive sequential implementation were quite promising. In Kapritsos' doctoral defense, he reported that the throughput of this speculative executing Adam system steadily increased with the number of cores according to the benchmarks that he ran. Even though the throughput increased, it did not seem to be scaling particularly linearly with the number of cores. Servers with 16 cores running 16 execution threads in this implementation of Adam only had about 4x the throughput of the same servers running with a single thread each.

The single-threaded pipelining technique described above also showed serious promise. For very short client requests (0.1ms each), the service managed to even more than the expected 2x performance gain, probably due to the backend service batching requests creating what acted in practice like an additional pipeline stage. For requests that took longer to process (1-10ms each) however, the system successfully achieved an approximately 2x throughput increase.


\chapter{Multi-Threaded Pipelining in Adam}

The core of this work was to adapt the previous work performed on the Adam project to support both multi-threading and pipelining simultaneously. As discussed in chapter 3, initial work on Adam had yielded some promising results both in the multi-threaded non-pipelined case and the single-threaded pipelined case. In order to better understand our approach to combining these techniques. Let use first define a couple terms for clarity:
\begin{itemize}
\item A \textbf{batch} of requests is a set of mostly non-conflicting client request to execute.
\item A \textbf{group} of threads are threads that will run at the same time in the pipeline.
\end{itemize}

At a very high level, our approach was to adapt the existing single-threaded pipelining code to have it schedule groups of threads to run in the pipeline instead of a single thread. To manage each group of threads, we adapted the existing multi-threaded non-pipelined Adam code to allow it to behave within this pipelined environment. The following chapter will explain at length the steps we took to convert the existing code we started with, as well as the new code we had to write to make this idea a reality

\section{Theoretical Benefits}

By combining both pipelining and multi-threading, we expected our speedups over a naive replicated state machine in this system model to both scale linearly with the number of threads we were using as well as double because of the two stage pipeline. In total this meant that with the 16 core machines we had to test with, we hoped to see a 32x throughput improvement in our microbenchmarks.

\section{Initial Code Structure}

\subsection{Sequential Pipeline Manager}

The Sequential Pipeline Manager (SPM) was first implemented by Manos Kapritsos to demonstrate that pipelining had promise in this system model. The SPM in particular was the part of the system that controlled what thread was currently running and handled scheduling the next thread when the running thread desired to yield. This code was very straightforward but it served as the inspiration for the pipeline manager that we built later to schedule groups of threads.

The SPM began execution by deterministically assigning requests to each thread. Note that even though the pipeline executed sequentially, the SPM leveraged lightweight threads to simplify the scheduling process. After assigning work, the SPM notified the first thread to start. That thread would then start processing the client request until the point that it was ready to make the nested request. The running thread at this point would call a yield function on the SPM to notify it to schedule the next thread and then the running thread would immediately make a blocking call to the backend service. When the SPM received this yield notification, it notified the thread with the next higher id than the previous running thread (modulo the number of threads). Eventually, the last thread with work to do would yield the pipeline, at which point the SPM scheduled the first thread again. 

This first thread would then either continue waiting for the nested request to complete, or would continue executing the client request to completion. When the thread completed, it notified the SPM to inform it of its progress and of the fact that it no longer needed to be scheduled. Once all threads had responded in this way, the SPM finished the batch and released the results to the clients.

\subsection{Existing Multi-threaded Adam Implementation}

% Maybe rename this to core components or something?
\section{Modifications and New Code}

\subsection{The Parallel Pipeline Manager}

\subsection{The Parallel Group Manager}

\subsection{Verification Stage}

\section{Rollback and Correctness Guarantees}

\section{Evaluation and Results}

When we finished implementing Adam as described above, we were dismayed to find that our throughput increase numbers were much lower than we had hoped. In fact for a variety of reasons, we first saw that using 8 threads in Adam only resulted in less than a 4x throughput increase over the simple sequential implementation. We then took a methodical journey through several heinous bugs and the intricacies of this design until we arrived at much more satisfying performance results. For details on the approach we took to correcting our implementation, and more about the problem that we are trying to solve in general, please read Jim Given's thesis.

\chapter{Conclusion}

\end{document}
