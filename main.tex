\documentclass[11pt, oneside]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{float}
\usepackage{graphicx}

\title{Communicating Replicated State Machines}
\author{David Wetterau}
\date{April 2015}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}\label{Introduction}
Computing systems today are rarely self-contained units. 
The desire for more levels of abstraction and cleaner separation of responsibilities has led to systems being modularized into services that must communicate and work together to accomplish tasks. 
Not only is this separation observed within single systems, but also in systems that interact with external services such as cloud based storage or computing providers. 
In this paper we will present a way to build these systems in a highly available way by leveraging existing techniques such as the replicated state machine abstraction.

Unfortunately, the replicated state machine abstraction as it exists today is not immediately fit for this multiple service environment. 
Previous work on the RSM abstraction has focused on the client-server model, where the RSM behaves as a single-correct-server from the perspective of the client and nothing else. 
This behavior does not allow these systems to communicate with other services because doing so could expose the intermediate state of the system which is not guaranteed to be consistent in this abstraction.

Aside from consistency issues, a traditional RSM abstraction would also suffer from fundamental performance deficiencies in this environment. 
Most replication protocols rely on the sequential execution of commands to guarantee that replicas operate in a deterministic manner. 
Not only does sequential execution do a poor job of leveraging abundantly available multicore machines, it also requires that a system in this environment remain idle while waiting for the response from a command issued to another service.

In this paper, we will present a technique to adapt the replicated state machine abstraction to the environment described above. 
We will first leverage recent work on multi-threaded speculative execution to allow our system to better utilize the multicore nature of machines today. 
In addition, we will show how to pipeline a system using speculative execution to mitigate the blocking of the system while waiting for the response of communications it may perform. 
This work builds on operations originally made by the Laboratory for Advanced Systems Research at UT and was a joint effort between myself and Jim Given. 
I highly recommend reading Jimâ€™s thesis on this same work to get the complete understanding of this project.

The rest of this paper is structured as follows. 
Chapter \ref{Background} will provide a comprehensive background on the replicated state machine abstraction and the Eve system, which pioneered the use of speculative execution within a RSM. 
Chapter \ref{Adam} will introduce Adam, a system to provide high availability in an environment with services that communicate not only with clients, but also with other services. 
Chapter \ref{AdamDesign} will describe at a high level the design and benefits of pipelining in Adam. 
Chapter \ref{AdamImplementation} will explore our implementation of Adam in detail along with the correctness guarantees Adam provides. 
Chapter \ref{AdamResults} will display our evaluation and the performance of Adam. 
Chapter \ref{RelatedWork} will discuss work related to Adam and the conclusion of this thesis will be presented in Chapter \ref{Conclusion}.

\chapter{Background}\label{Background}
\section{State Machine Replication}
In order for a distributed system to be highly available in the presence of failures, the system must be replicated. 
State Machine Replication (SMR) is a way to guarantee that multiple replicas, either virtual or physical, produce the same outputs when given the same commands. 
In this section we will explain the abstraction of a replicated state machine along with how SMR has been traditionally implemented.

At the core of SMR is a deterministic state machine. 
A state machine is a model of computation that consists of state variables, which store the state of the machine, and commands that transform this state. 
In an SMR implementation, requests are issued by clients and contain the name of the state machine to operate on, the command to perform, and any other information needed to perform the command. 
When a replica receives a request, it performs the command by transitioning between states and finally producing output. 
For a write request from a client, the state machine might end in a state different from where it began (this can be thought of as the value of some variable being updated). 
In the case of a read request, the state machine returns to the same state it started at (no state was really changed).

The correctness goal of SMR is to provide a system that retains both safety and liveness in the presence of replica failures. 
SMR can be adapted to work under many different failure models simply by adjusting the number of replicas. 
For a weaker failure model, one only needs to either increase the number of replicas in the system or decrease the number of tolerated failures accordingly. 
In order to provide correctness, the requirement of SMR is that the observable state (including but not limited to the outputs) of each state machine must always be the same across all replicas. 
This is simply because if all replicas have the same observable state, any replica on its own could communicate with clients if another crashed.

To provide this correctness requirement, SMR has traditionally been implemented by only allowing sequential execution of commands. 
The reason behind this is that if commands are first ordered the same way on each replica, and are then executed one at a time in this deterministic way, then all replicas will arrive at the same state. 
This is easy to see because each state machine simply follows the exact same state transitions in the same order. 
This design is referred to as the agree-execute approach, and has been applied in a wide range of systems including the original Paxos protocol.

Sequential execution was an acceptable option when SMR was first introduced since single core performance was rapidly increasing. 
Now that the world has instead shifted to increasing the amount of parallelism to improve performance, we find that sequential execution is a serious limitation. 
If we tried to naively relax this sequential execution restriction and run commands using multiple threads, there would be no guarantee that the replicas would arrive in the same final state since the commands could be interleaved across the threads. 
In the next section, we will see one way that we can still provide the replicated state machine abstraction without the need for this sequential execution restriction. 
 

\section{Eve: Execute-Verify Replication}

In the previous section, we explored how the majority of SMR implementations relied on the agree-execute architecture to ensure that replicas converged to the same ending state. 
This approach guarantees convergence through determinism and sequential execution. 
One observation that can be made about these approaches is that they have effectively reduced the actual safety requirement of SMR (that the observable state of each state machine is the same across all replicas) to the problem of guaranteeing that all replicas agree on an order of commands to execute. 
Eve is a system that was developed at the University of Texas at Austin that uses the real safety requirement of SMR to enable multi-threaded execution.

Eve uses what is referred to as speculative execution in a way that provides the safety guarantee of SMR while still providing liveness to form a correct replicated state machine abstraction. 
In Eve, replicas are given a set, or batch, of requests to execute from the primary, however they are not required to agree on an order in which to execute them. 
Replicas then fully leverage the potential of their underlying multicore machines to execute all of the requests in parallel. 
At the end of the execution, replicas compare states with each other in a verification stage and if enough replicas arrived at the same state, the results of the client requests are released. 
This step can be thought of as resolving the speculative execution. 
If not enough replicas agree on the final state, it must be due to a non-deterministic interleaving of the requests (since other sources of non-determinism are eliminated), so each replica simply re-executes all of the requests in the group in a deterministic order sequentially to guarantee both safety and liveness. 
This process of speculatively executing requests in parallel and then verifying that all replicas arrived at the same ending state is referred to as execute-verify.

In theory the execute-verify approach is straightforward, but in order for Eve to work well, it is important that the case that requests have to be re-executed in the sequential manner is rare. 
If this were not the case, Eve would be strictly worse than SMR implemented sequentially because of the need to execute requests multiple times to guarantee safety. 
To reduce the likelihood that request interleavings cause the final states of replicas to diverge, the authors introduce what they call the mixer which is discussed in \ref{EveMixer}. 
After Eve runs the mixer on the incoming requests, it then proceeds to the execution stage which is described in \ref{EveExecution}. 
After executing a batch of requests, Eve moves on to the verification stage which guarantees that replicas compare their states efficiently and in a way that tolerates even Byzantine failures. 
This verification stage is described in \ref{EveVerification} and the final results that authors were able to produce with Eve are presented in \ref{EveResults}.

\subsection{The Mixer}\label{EveMixer}
Given a set of requests submitted to the Eve system by clients to execute, the mixer constructs a batch of requests to schedule for execution in the following way. 
The mixer examines each request to determine if it conflicts with any request already added to the next batch to execute. 
If the request does not conflict, it adds the request to the next batch to execute. 
If it does conflict, the mixer passes over the request. 
After a complete pass, the mixer releases the batch to be executed and after doing so, repeats this process on the requests that were left out of the batch on the previous pass.

The authors found that mixers were both easy to implement for various real applications such as the TPC-W benchmarks, and greatly reduced the likelihood that replicas would diverge due to thread interleavings. 
It is important to note that while the mixer does improve performance of the system greatly, it is not needed for either the safety or liveness conditions of SMR. 
Because Eve can re-execute requests as a fallback, there is no requirement that the requests in a batch do not conflict and therefore there is no required accuracy of the mixer in order for Eve to be correct.

\subsection{The Execution Stage}\label{EveExecution}

The execution stage in Eve is where the system gains its massive performance gains over a sequential agree-execute SMR system. 
The output of running the mixer on the set of client requests to execute is a batch of requests that are hopefully unlikely to cause problems when they are executed in parallel. 
In the execution stage, these requests are distributed out across a number of execution threads to be run in parallel at the same time. 
When all of the execution threads finish performing the commands specified by the client requests in the batch, a hash of the final state of the state machine is computed efficiently using a concurrent Merkle tree implementation. 
This hash of the final state is then used in the verification stage to determine whether or not the parallel execution caused a divergence in the final states of the replicas.

\subsection{The Verification Stage}\label{EveVerification}
In Eve's verification stage, replicas must compare their final states in an efficient manner that is also resistant to even byzantine failures. 
This is accomplished by relying on cryptographic primitives such as Message Authentication Codes (MACs) and collision resistant hashes of the states. 
As mentioned in the previous subsection, Eve replicas compute a hash of their state after processing a batch of requests by each using a Merkle tree for efficiency. 
After computing the root hash of the tree, replicas sign and send to a separate set of verification machines a signed token that consists of the root hash they started with before processing the current batch, and the hash of their state after processing the batch in a parallel manner.

The verification machines then run an agreement protocol on the tokens submitted to them by all the replicas. 
In the simple case, all of the tokens match and the verifiers instruct the primary to release the result of the computation to the clients. 
This is achieved by responding with a ``commit" message. 
If instead some tokens differ but there is a token that enough replicas submitted to guarantee that a correct replica submitted it, the verification machines respond with a commit for that token but require the replicas that did not reach that state to receive a state transfer from the replicas that did. 


In the worst case, the verifiers do not identify a token that was reached by enough replicas. 
In this case, the verifiers require all replicas to re-execute the requests in the batch in a sequential manner. 
After the replicas have re-executed the requests in the same order, there is no need to enter the verification stage again because the execution was performed deterministically. 
This deterministic execution guarantees that all replicas arrive at the same ending state. 
Note that this verification stage maintains the liveness property, since it is impossible for the replicas to get stuck in an infinite loop of verification.

\subsection{Performance Results}\label{EveResults}

The experimental evaluation of Eve resulted in promising results for this approach to a multi-threaded replicated state machine. 
In particular, the creators of Eve demonstrated with microbenchmarks that Eve was capable of a 12.5x speedup over sequential execution using 16 core machines with 10ms requests. 
These numbers did however fall slightly with lightweight requests. 
They found that the improvement decreased to 10x for 1ms requests and 3.3x for 0.1ms requests mainly due to the inability to saturate the system with client requests and the increased overhead relative to the time spent performing computation for each request.

The authors also compared Eve's performance to an existing attempt at performing multi-threaded SMR called Remus. 
In Remus, the primary replica executes requests with multiple threads and then transfers its entire state to the other replicas. 
This approach, referred to as passive replication, both requires considerably more bandwidth (due to the transfer of full states) and does not protect against commission failures, while Eve does. 
Eve was shown to outperform Remus by a factor of 4.7x while using orders of magnitude less network bandwidth in the authors' tests.

\chapter{Adam: State Machine Replication with Interacting Services}\label{Adam}

In the last chapter we saw that the replicated state machine abstraction can be used with in a parallel environment with serious performance improvements in terms of request throughput. 
Systems today are rarely well described by the simple client-server model.
Instead, most services need to interact with other services while processing a request from a client. 

For example, if a client visits the website Dropbox.com, the server handling the client's request needs to make what we will refer to as a ``nested request'' to a separate service. This nested request functions to retrieve the names of the client's files, before sending the contents of the webpage back to the client. Figure \ref{anatomy} shows the anatomy of a client request that we will be considering for the for the rest of this paper. In the figure, you can think of the local computation as the time that the webserver spends rendering the HTML to return and the remote computation as the remote service loading the user's files for this example.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{RequestAnatomy.png}
\caption{\label{anatomy}The anatomy of a client request that must perform a nested request. The dark line in the middle will be used to indicate a checkpoint operation. We will say that a request requires $2$ time slices to complete for simplicity: the first for the local computation and an equal amount of time for the remote computation.}
\end{figure}

Eve's approach to multi-threaded SMR successfully leverages the multicore nature of today's machines, but it is not immediately clear how to extend such a system to allow for replicated services to communicate. 
In this chapter, we will explore the previous work that was performed to convert the Eve system to work in this new environment: one where servers not only expose their states to clients, but also to other services through nested requests.

\section{Initial Approaches}

Before describing Adam, it is important to understand why speculative execution approaches such as Eve do not work in this environment with communicated services. 
Consider a service $A$ that is using Eve and is processing client requests. 
While processing some of the client requests service $A$ receives, it must make a call to service $B$. 
Now suppose that due to the speculative nature of Eve, too many replicas diverge while processing this client request. In Eve, this causes all replicas to rollback to their state before they began processing the client requests and try again in a sequential manner. 
At this point, the request that service $A$ made to $B$ has already happened and cannot be rolled back. 
If service $B$ had clients of its own, they could see the result of this nested request even after service $A$ tried again. 
Worse still, when service $A$ re-executes the requests sequentially, there is no guarantee that it makes exactly the same nested request. 
Non-determinism caused by thread interleaving could result in a nested request made as a result of the same client request that is different from the nested request that would be made when processing sequentially.

This well known problem is known as the output-commit problem and a very simple solution to the problem is to always take a snapshot before producing output or exposing internal state to any outside observer. 
The first attempt to convert Eve to work in this communicating services setting took exactly this approach, and is described in \ref{EveModification}. 
A separate idea to achieve higher performance in this setting was to simply run in a deterministic and sequential manner, but to leverage deterministic pipelining for increased performance. 
With this idea, Adam would also not be speculative and no rollback would ever be needed to maintain correctness. This second approach is described in \ref{STP}.

\subsection{Naive Sequential Implementation}

The simplest way to imagine implementing SMR in this environment would be to use the typical agree-execute approach.
This approach would work, however we will see in the next few sections that we can do much better. 
Figure \ref{NaiveSequential} shows what such an implementation would look like if it were executing $12$ client requests. 
Since agree-execute does not use speculative execution, no checkpoints are needed in this approach but you can clearly see that while remote computation is happening, no local computation can be performed.
As a result, the execution of $12$ client requests that require an equal amount of work on the local and remote services (a simplification we will use for ease of explanation) takes $24$ time slices.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{NaiveSequential.png}
\caption{\label{NaiveSequential}An example execution of $12$ client requests with a naive sequential implementation. No checkpoints are needed for this approach.}
\end{figure}

\subsection{Eve Modification}\label{EveModification}

In an effort to leverage the performance gains that speculative execution approaches such as Eve promised, the first attempt to construct Adam focused on solving the output-commit problem that Eve faced in this environment. 
To do this, the initial Adam contributors at UT Austin simply modified the existing Eve implementation to take checkpoints before making nested requests. 

Consider an Eve system with two execution threads processing client requests in a speculative manner. 
When the execution threads arrive at the part of processing the client request that requires a nested request to another service, they inform the system of their progress and wait. 
After the last thread reaches such a point in the request processing, the system computes a hash of its state in exactly the same way that Eve ensured all replicas were the same at the end of processing an entire batch. 
If all replicas were in the same state after processing these prefixes of the client requests, the threads could safely make their nested requests and continue executing until the next such time, or until the end of their last request.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{Parallel.png}
\caption{\label{parallel}An example execution of $12$ client requests with the modified Eve parallel approach. For $12$ requests on $2$ execution threads, $12$ time slices are used and $7$ total checkpoints are performed.}
\end{figure}

Figure \ref{parallel} Shows what an execution of $12$ client requests would look like with this implementation using $2$ execution threads. 
Notice that checkpoints are taken right before the execution threads begin to make their nested requests to the remote service. 
Also notice that an additional checkpoint is needed at the end to make sure all replicas converge at the end of the batch.
This implementation would finish the batch of $12$ requests in just $12$ time slices compared to the $24$ that the naive approach requires.

\subsection{Single-Threaded Pipelining}\label{STP}

A completely separate way to solve the output-commit problem in this environment is to simply not execute in a speculative manner. 
The original contributors to Adam also implemented a system that leveraged single-threaded deterministic pipelining to try and solve this issue.

The advantage of pipelining in this environment is that when a single thread is processing client requests and makes a request to a separate service, it must wait synchronously for the response. 
If this communication with the remote service takes as long as the computation that the first service must perform, the system would be idle half the time.
It is then clear that if it were possible to perform more work while waiting for the nested request's response, the performance of the system would increase.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{SequentialPipelined.png}
\caption{\label{seqpipe}An example execution of $12$ client requests with the single-threaded pipelining approach. For $12$ requests on $2$ execution threads, $13$ time slices are used and no checkpoints are performed because all execution is deterministic.}
\end{figure}

In order to build a system that performed this pipelining, the researchers used multiple threads but only ran them one at a time and scheduled them in a deterministic way. 
Deterministic scheduling and only having a single execution thread running at any point in time guaranteed that all requests were executed deterministically, and therefore that all replicas in the system converge without the need for checkpointing or potentially rolling back.

In figure \ref{seqpipe} you can see the execution of $12$ client requests in a single-threaded pipelined system. Notice that the grey rectangles indicate local computation, or that the replica machine is actually performing work locally. Only one thread is ever performing local computation at a time (guaranteeing determinism), but because of pipelining, the total number of time slices is cut almost in half over the naive approach.

\section{Initial Results}

The initial findings in terms of the performance gain that applying speculative execution had over a naive sequential implementation were quite promising. 
In Kapritsos' doctoral defense, he reported that the throughput of this speculative executing Adam system steadily increased with the number of cores according to the benchmarks that he ran. 
Even though the throughput increased, it did not seem to be scaling particularly linearly with the number of cores. 
Servers with 16 cores running 16 execution threads in this implementation of Adam only had about 4x the throughput of the same servers running with a single thread each.

The single-threaded pipelining technique described above also showed great potential. 
For very short client requests (0.1ms each), the service managed to achieve even more than the expected 2x performance gain, probably due to the remote service batching requests creating what acted in practice like an additional pipeline stage. 
For requests that took longer to process (1-10ms each) however, the system successfully achieved an approximately 2x throughput increase due to the 2 stage nature of the pipeline: the first being the local computation and the second being the computation on the remote service.

\chapter{Multi-Threaded Pipelining in Adam}\label{AdamDesign}

\section{Design Overview}

The core of the work that we performed was to adapt the previous work on the Adam project to support both multi-threading and pipelining simultaneously.
As discussed in Chapter \ref{Adam}, initial work on Adam had yielded some promising results both with the multi-threaded non-pipelined approach and the single-threaded pipelined approach. 
In order to better understand our approach to combining these techniques, let use first define a few terms for clarity:
\begin{itemize}
\item A \textbf{batch} of requests is a set of mostly non-conflicting client (the output of a mixer as defined in \ref{EveMixer}) request to execute.
\item A \textbf{group} of threads are execution threads that will run at the same time in the pipeline.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{PipelinedParallel.png}
\caption{\label{parpipe}An example execution of $12$ client requests in the multi-threaded pipelining approach we implemented. Two groups of two execution threads each can process $12$ client requests in just $7$ time slices.}
\end{figure}

Figure \ref{parpipe} shows what an execution of $12$ client requests would look like in our multi-threaded and pipelined system with two groups of two execution threads each. 
Notice that only one group is performing local computation at a time (the dark rectangles) but multiple threads within a group are running simultaneously.
Also notice that the full execution of all $12$ client requests requires only seven of the same time slices used in the other diagrams.
This abstraction does not however effect the total number of checkpoints that must be performed (seven) from the original multi-threaded approach.

At a very high level, our approach was to adapt the existing single-threaded pipelining code to have it schedule groups of threads to run in the pipeline instead of a single thread. 
To manage each group of threads, we adapted the existing multi-threaded non-pipelined Adam code to allow it to behave within this pipelined environment. 
The following chapter will explain at length the steps we took to convert the existing code we started with, as well as the new code we had to write to make this idea a reality.

\section{Theoretical Benefits}

By combining both pipelining and multi-threading, we expected our speedups over a naive replicated state machine in this system model to both scale linearly with the number of threads we were using as well as double because of the two stage pipeline. 
In total this meant that with the 16 core machines we had available to test with, we hoped to see approximately a 32x throughput improvement in our microbenchmarks. 

The figure \ref{parpipe} shows the improvement that we would general expect by using just two execution cores (because only two threads are ever running locally at a time.
We would expect a 2x throughput increase from using two threads concurrently as well as approximately a 2x throughput increase from the two stage pipeline for about a 4x improvement overall.

Visually, you can see this exemplified in the number of slices in each diagram. 
The naive sequential implementation (Figure \ref{NaiveSequential}) required $24$ time slices while the execution in our system required only $7$.

\begin{align*}
24 / 7 \approx 3.43
\end{align*}

This number actually converges to $4$ as the number of requests run in each batch increases to infinity.
A formal proof of this fact is available in Jim Given's thesis but conceptually you can see that the larger the number of requests that each thread must execute, the less that the ``unmatched'' slices at the beginning and the end of the batch matter. 
If the diagrams instead showed batches of $24$ requests each, the naive implementation would require $48$ time slices ($24$ requests $\cdot \ 2$ slices / request $=\ 48$).
Our pipelined implementation would require only $13$ slices: $24$ requests / $4$ threads $ = 6$ requests / thread $\cdot \ 2$ slices / request $ = 12$ slices $ + 1$ extra slice of time for the first two threads at the end of the batch. 

\begin{align*}
48 / 13 \approx 3.69
\end{align*}

\chapter{Adam Implementation}\label{AdamImplementation}

\section{Initial Code Structure}\label{existing}

When we began our contributions to the Adam project, implementations existed for both approaches described in Chapter \ref{Adam}. 
Before explaining the work that we did to create a pipelined SMR system that used speculative execution, we will examine in detail that existing building blocks that we used to construct our final implementation.

\subsection{Existing Multi-Threaded Adam Implementation}\label{BM}

The existing multi-threaded Adam implementation we started with was a modification to the existing Eve code that allowed the code to perform nested requests. 
Some of the core changes involved the use of Apache's Commons Javaflow library to easily allow execution threads to rollback their state to a checkpoint, even if they are currently in the middle of executing. 
The other modifications to Eve involved adding support for multiple checkpoints taken per batch, one after every time the execution threads ``hit a wall'', meaning that all currently active execution threads were either finished with the entire batch of requests or were ready to make a nested request.

Execution threads were controlled in this implementation by the Batch Manager (BM) which handled the logic of making sure all threads had ``hit a wall'' and initiated the checkpoint operation for each execution thread. 
This BM was also intended to handle ensuring that the all replicas were at the same state at each wall, which involved sending off a hash of the replica's state and initiating a rollback if needed. 
When we received the code however, this work was largely not finished as the existing multi-threaded attempt was intended to simply demonstrate that speculative execution could still provide performance gains in this environment.

\subsection{Sequential Pipeline Manager}\label{SPM}

The Sequential Pipeline Manager (SPM) was first implemented by Manos Kapritsos to demonstrate that the single-threaded pipelining approach had promise in this system model. 
The SPM in particular was the part of the system that controlled what execution thread was currently running and handled scheduling the next thread when the running thread desired to yield.
This meant that the SPM was also responsible for enforcing the condition that only one execution was running at any point in time and that all execution was deterministic.
This code was very straightforward but it served as the inspiration for the pipeline manager that we built later to schedule groups of threads.

The SPM began the execution of a batch of requests by deterministically assigning requests to each thread.
Note that even though the pipeline executed sequentially, the SPM used many lightweight threads to simplify the scheduling process. 
After assigning work, the SPM notified the first thread to start. 
That thread would then start processing the client request until the point that it was ready to make the nested request. 
The running thread at this point would call a yield function in the SPM to notify it to schedule the next thread and then the running thread would immediately make a blocking call to the remote service. 
When the SPM received this yield notification, it notified the thread to begin execution again with the next higher id than the previous running thread (modulo the number of threads). 
Eventually, the last thread with work to do would yield the pipeline, at which point the SPM scheduled the first thread again. 

This first thread would then either continue waiting for the nested request to complete, or would continue executing the client request to completion. 
When the thread completed processing the current request, it notified the SPM to inform it of its progress and of the fact that it no longer needed to be scheduled. 
Once all threads had responded in this way, the SPM finished the batch and released the results to the clients.

% Maybe rename this to core components or something?
\section{Core Components}

When implementing Adam with the multi-threaded pipelined approach, we decided to first combine the two existing approaches into one system before implementing the parts of the functionality that were not yet present. 
We began by adapting the Sequential Pipeline Manager to schedule groups of threads instead of just one at a time, and then modified the Batch Manager to control a group of threads and behave within the pipeline.
After these modifications, we finished implementing parts of Adam that had not been implemented for the initial proof of concepts such as the verification stage and made appropriate modifications to other existing parts of the code base to support both multi-threading and pipelining simultaneously.

\subsection{The Parallel Pipeline Manager}

By taking the structure of the existing SPM written by Kapritsos (and described in \ref{SPM}), we built what we call the Parallel Pipeline Manager (PPM) to schedule groups of threads in the context of the pipeline. 
The changes that we made to the SPM to control groups of threads mostly centered around initialization and the detection of the correct time to swap groups on or off the execution pipeline.

Within the SPM, a simple mechanism was used to schedule client requests to execution threads that ran one at a time in a pipelined fashion. 
Essentially, the SPM would assign the requests to threads by giving each thread one request before giving the first thread a second request and so on. 
In the PPM, we instead have to schedule requests within a batch to both groups of threads and to threads within those groups. 
This is performed after the mixer has been run on the batch, so we are able to assume that requests are mostly non-conflicting. 
We experimented with a couple of ways of assigning the requests to the threads but eventually found that our system works best when we assign every thread in the first group a request before beginning to assign requests to threads in the second group.
We continue this process with the remaining groups before assigning a second request to each thread within the first group. 
We believe that this is the best way to assign the requests because assigning them in this way makes sure that all threads are utilized as best as possible, leveraging the multicore nature of the machine before taking advantage of the pipelining by using multiple groups.

The SPM had a simple method that execution threads could call when the execution thread was about to make its nested request and was ready to yield the pipeline to the next thread. 
In the PPM, this procedure is slightly more complicated because the PPM must wait for all threads to be in a state that they can yield the pipeline in. 
Each execution thread makes the same sort of yield request, but the result of the function call is simply a flipped bit in a bitarray that stores which threads are ready to yield. 
When the last thread in the currently active group (the group that the PPM is allowing to execute) calls this yield function, the last thread notifies the PPM's control thread.
The PPM control thread then realizes that it is time to schedule a new group of threads to run. 

This bitarray also has to be updated when threads no longer want to yield and are ready to execute again (e.g. when the nested request returns for a thread). 
We decided in our implementation to only allow the PPM's control thread to mark when threads no longer want to yield because of the following scenario. 

Imagine that in a group of two execution threads, the first thread reaches the point where it is ready to yield and makes its nested request. 
Now imagine the second thread require more local computation time and therefore is slightly behind the first thread in making its nested request.
In this situation the second thread does not reach the point to yield before the nested request that the first thread made returns.
If we allowed the first thread to mark that it no longer wanted to yield the pipeline (since its nested request had completed), the PPM would not schedule the next group of threads to execute because not all of the threads in the group wanted to anymore.
This is an issue because the second request's remote computation could take a long time, in which case no thread is currently executing on the local machine.
A simple workaround for this is to only allow the PPM to mark the threads as not wanting to yield after they have been swapped off the pipeline at least one time.

\subsection{The Parallel Group Manager and Execution Threads}

The second part of our system that allows groups of threads to run in a speculative manner is the Parallel Group Manager (PGM). This part of the system that controls the execution threads within a single group.
It is at this layer of abstraction in our system that we handle taking checkpoints, possibly rolling back, and informing the PPM when all of the threads in the group are finished executing their assigned requests within the batch.
The Batch Manager (described in \ref{BM}) served as our starting point for this part of the implementation, as it was already designed to control multiple execution threads in the Adam environment.
Some of the major modifications we had to make to the existing BM code involved making sure that the PGM was able to be scheduled on and off the pipeline controlled by the PPM, and that verification was performed in a way to guarantee safety and liveness. 
This involved making sure that verification requests were made at all of the required times and that the responses to these messages were handled properly. 
More details about our verification modifications and how we handled rollback are described in \ref{Verification}. 

In order to make sure the PGM did not continue executing after being swapped off the pipeline, we simply had it communicate with the PPM to determine what the currently active group was. 
If the active group corresponded to the group controlled by the PGM and its corresponding control thread, the PGM continued instructing the execution threads to process more requests and checkpoint when needed. 
If instead the PGM found that the current group was no longer its own, it simply waited until the PPM notified it to continue.

We also had to make many changes to the execution threads to ensure that they behaved in this new parallel pipelined environment.
In particular, we had to ensure that the execution threads made the appropriate calls into the PPM to notify the PPM when they were ready to yield and that the threads ensured that it was their corresponding group's turn to execute in the pipeline before proceeding with their executions.

\subsection{Verification Stage and Rollback}\label{Verification}

When we inherited the existing proof of concept Adam implementation, the verification stage had yet to be converted from its original behavior within Eve.
In Eve, the system was only required to send out a verification message at the end of the computation of a batch to ensure that all replicas converged.
In Adam however, the verification process must be performed before any execution thread makes a nested request as well as at the end of the batch because both of these situations expose internal state to potential clients.

In order to make this modification, we first had to add a second message processing thread to the system that could concurrently process the responses from a verification stage while also processing incoming client requests. 
This was mostly an implementation detail but we do suspect that each control thread we added to the system reduces the scalability of the system as we approach maximum utilization of our machines.
When the verification response is received from the verifiers by the execution replicas, they process the response and determine whether they need to rollback, receive a state transfer, or could safely continue executing in the same way. 
This part of the verification stage works almost identically to the way Eve's worked which is described in \ref{EveVerification}.
Details about how our system handled rollback responses to verification requests can be found in the next section.

\section{Rollback and Correctness Guarantees}

In this parallel pipeline system we must be prepared to rollback the state of the replica to one that all replicas agree on in the event that the speculative execution causes a divergence. 
Note that because we always ensure that all replicas are in the same state immediately before making any nested request, this means that the state that we must rollback to is right before some execution threads make a nested request or right at the beginning of the batch. 
If it is the later, rollback is trivial, we simply must throw away all work performed so far and execute in a sequential deterministic manner to guarantee that progress is made and that all replicas converge. 
This deterministic execution can even be performed in a sequential pipelined manner to achieve better performance than a naive sequential approach.

If instead we rollback to right before the threads in a group make a nested request, our system must restore the state of all execution threads to the state they had before making these requests and switch into a sequential deterministic mode to ensure progress and convergence. 
We use Apache's Commons Javaflow library, specifically the Continuations objects in this library, to keep track of the stack of each execution thread.
This allows us to restore this stack and guarantee that all execution threads are in the exact state they were in right before they made their nested request on each replica. 
These Continuation objects with the stack of each thread are stored in the same Merkle tree data structure that Eve used which allows the replicas to make sure that they also have the same recorded stacks for each execution thread whenever a verification is performed. 
This also allows replicas to receive continuations in a state transfer and immediately start the execution threads at the same point in the execution. 

After performing a rollback in the non-trivial case, the threads in the now current group will either re-execute their nested request (if a response was never received) or will read the result from a cache if a response was received.
Since we know that all execution threads were in the same state before making this request, we know that the request that each thread makes must be exactly the same, thereby eliminating the issue that Eve has in this environment with making requests that can never be repeated after rolling back.

\section{Future Work}

One proposed improvement to the above design that we did not implement was an optimization in the verification process.
The idea is to combine the nested requests with the verification requests, eliminating the need for the verifiers and this extra round trip.
This approach does however require that the remote service is aware that the service making nested requests is running our Adam system while our current implementation does not. 
In practice, we found that this round trip was not the bottleneck of the system, but this improvement could still easily be made.

As shown in the evaluation section, we did find that our system had much better performance gains on longer client requests than shorter ones due to the overhead that the additional checkpoints incurs in the form of additional Merkle tree computations. 
We believe that the system could be modified to minimize the total number of checkpoints needed by making much larger groups and not checkpointing until all of the requests in these larger groups are ready to make the nested request.

\chapter{Evaluation and Results}\label{AdamResults}

When we finished implementing Adam as described above, we were dismayed to find that our throughput increase numbers were much lower than we had hoped. 
In fact for a variety of reasons, we first saw that using 8 threads in Adam only resulted in less than a 4x throughput increase over the simple sequential implementation for very large requests. 
We then took a methodical journey through several heinous bugs and the intricacies of this design until we arrived at much more satisfying performance results. 
For details on the approach we took to correcting our implementation, and more about the problem that we are trying to solve in general, please read Jim Given's thesis.

The evaluation presented here was performed on a testbed of several Dell PowerEdge R200 4x 2.4GHz Intel Xeon machines and three Dell PowerEdge R515 16x 3.2GHz AMD Opteron machines. 
We used the 16 core (R515) machines as our execution replicas and the 4 core (R200) machines as our verifier, client, and filter machines. 
Because we only had three 16 core machines, we were only able to perform scaling studies up to 16 cores in an unreplicated setting (meaning we tolerated no faulty execution replicas) with one 16 core machine as the local replica and another as the remote service. 
For these unreplicated studies, only the execution nodes were not replicated, all verifier machines were still replicated in the same way.
The other studies described in this setting were performed by using each 16 core machine as both a 8 core local replica and an 8 core remote replica which constrained us to only being able to test in a full setting (with $f = 1$) with up to 8 execution threads.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{RequestAnatomy.png}
\caption{\label{scaling}Adam speedup over single group, single thread performance for small, medium, and large requests.}
\end{figure}

One of our chief concerns when evaluating the performance of our system was to see how the system scaled with additional parallel execution threads. Figure \ref{scaling} shows the factor of throughput increase that we achieve by adding more execution threads to our system in an unreplicated environment. 
These numbers were obtained by running our system with three groups of $n$ threads each where $n$ is the number found on the horizontal axis. 
We then took the average saturated throughput of the system for each number of threads and divided it by the throughput that the system obtains with a single group of just one thread. 
As mentioned in Chapter \ref{AdamDesign}, we expected the theoretical maximum benefit of our system to have 32 times the throughput of the baseline, gaining a factor of 16 improvement from utilizing 16 cores and then doubling the throughput due to the two stage pipeline.
We come the closest to this theoretical maximum with large (10 ms local computation and 10ms remote computation) requests because the computation time helps mask the overhead introduced by the checkpointing and the verification stage.
We miss this mark by progressively more for shorter requests as the overhead becomes more pronounced.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{RequestAnatomy.png}
\caption{\label{scalingmedmerkletree}Adam speedup over single group, single thread performance for small, medium, and large requests.}
\end{figure}

The Merkle tree implementation that we used appears to be the main source of overhead in this system which can be seen in Figure \ref{scalingmedmerkletree}. 
In this graph, we compare the scaling behavior of the unreplicated system when using a real Merkle tree implementation to when we use a fake Merkle tree.
With the fake Merkle tree, we still perform the round trip for agreement, but all hashing and checkpointing operations are instant.
As a result, we believe that there is still room for improvement for requests that aren't as large through optimizations in how the Merkle tree. 
This could be through better detection of new objects in the replica that must be hashed or more intelligent space allocation within the tree itself.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{RequestAnatomy.png}
\caption{\label{head2head}Latency-Throughput graphs of Adam for large requests with all four approaches.}
\end{figure}

We also compared our Adam implementation head to head with the previous two approaches described in Chapter \ref{Adam} as well as a naive sequential implementation. Figure \ref{head2head} shows latency throughput graphs for our implementation running with three groups of eight threads each in a fully replicated environment. We compare our implementation in this graph to the multithreaded but non-pipelined approach running with 8 threads, and with the sequential-pipelined approach running with three threads (which is equivalent to our implementation using three groups).

\chapter{Related Work}\label{RelatedWork}

\chapter{Conclusion}\label{Conclusion}

We have presented a novel approach to providing the replicated state machine abstraction in an environment with communicating services that leverages both the multicore nature of today's machines and pipelining techniques to improve resource utilization. 
We found through the use of microbenchmarks that our design and implementation allows a workload of heavy requests to scale up to 21.8x the throughput of a naive sequential implementation in a non-replicated setting with 16 core machines, or up to 13.0x the throughput in a replicated setting with 8 cores per execution server.

\end{document}
